{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaVgIn5XTEpxShUuICtP6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhayanesh/bert-embeddingExtraction/blob/main/bert_embeddingsExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "3Jxjfhf3xHi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
      ],
      "metadata": {
        "id": "-Tev-tVpyoVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooCI8trpw34F"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "#reusing code for extracting updated indices from DLRM implementation\n",
        "def get_nonzero_grad_indices(embedding_layer):\n",
        "    nonzero_grad_indices = []\n",
        "\n",
        "    def hook(grad):\n",
        "        if grad.is_sparse:\n",
        "            grad = grad.to_dense()\n",
        "        unique_nonzero_indices = torch.unique(torch.nonzero(grad, as_tuple=False)[:, 0])\n",
        "        nonzero_grad_indices.extend(unique_nonzero_indices.tolist())\n",
        "\n",
        "    handle = embedding_layer.weight.register_hook(hook)\n",
        "    return nonzero_grad_indices, handle\n",
        "\n",
        "embedding_layer = model.bert.embeddings.word_embeddings\n",
        "nonzero_grad_indices, handle = get_nonzero_grad_indices(embedding_layer)\n",
        "\n",
        "for i in range(10):\n",
        "    input_text = \"Bidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced by researchers at Google. BERT is based on the transformer architecture. Specifically, BERT is composed of Transformer encoder layers\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    labels = torch.tensor([1]).unsqueeze(0)\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "updated_embedding_values = {}\n",
        "with torch.no_grad():\n",
        "    for index in nonzero_grad_indices:\n",
        "        updated_embedding = embedding_layer.weight.data[index].tolist()\n",
        "        key = 'bert.embeddings.word_embeddings.weight'\n",
        "        if key not in updated_embedding_values:\n",
        "            updated_embedding_values[key] = {}\n",
        "        updated_embedding_values[key][index] = updated_embedding\n",
        "\n",
        "print(updated_embedding_values)\n",
        "handle.remove()\n"
      ]
    }
  ]
}